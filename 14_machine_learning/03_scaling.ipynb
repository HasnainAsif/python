{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Standard Scaling\n",
    "\n",
    "`Standard scaling` is a method of scaling the data such that the distribution of the data is centered around 0, with a standard deviation of 1. This is done by subtracting the mean of the data from each data point and then dividing by the standard deviation of the data. This is a very common method of scaling data, and is used in many machine learning algorithms.\n",
    "\n",
    "The formula is as follows:\n",
    "\n",
    "z = (x - μ) / σ --> formula for **z-score**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an example dataset\n",
    "df = {\n",
    "    'age': [25,30,35,40,45],\n",
    "    'height': [165,170,175,180,185],\n",
    "    'weight': [55,60,65,70,75]\n",
    "}\n",
    "\n",
    "# conver this data to pandas datafram\n",
    "df = pd.DataFrame(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (-3, 3) is the range of the standard scaler\n",
    "\n",
    "# import the scalar\n",
    "scalar = StandardScaler()\n",
    "\n",
    "# fit the scalar on data\n",
    "scaled_df = scalar.fit_transform(df)\n",
    "# print(scaled_df)\n",
    "# print('-----------------------------------')\n",
    "\n",
    "# convert this data into a pandas dataframe\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=df.columns)\n",
    "scaled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# min-max scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (0, 1) is the range of the min-max scaler. Minimum is 0 and maximum is 1\n",
    "\n",
    "# import the scalar\n",
    "scalar = MinMaxScaler()\n",
    "\n",
    "# fit the scalar on data\n",
    "scaled_df = scalar.fit_transform(df)\n",
    "# convert this data into a pandas dataframe\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=df.columns)\n",
    "scaled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max ABS scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (0, 1) is the range of the max-abs scaler. Maximum is 1 and minumum >=0\n",
    "\n",
    "# import the scalar\n",
    "scalar = MaxAbsScaler()\n",
    "\n",
    "# fit the scalar on data\n",
    "scaled_df = scalar.fit_transform(df)\n",
    "scaled_df\n",
    "# convert this data into a pandas dataframe\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=df.columns)\n",
    "scaled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robust Scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust scaler: (-1, 1) is the range of the robust scaler\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# import the scalar\n",
    "scalar = RobustScaler()\n",
    "\n",
    "# fit the scalar on data\n",
    "scaled_df = scalar.fit_transform(df)\n",
    "scaled_df\n",
    "# convert this data into a pandas dataframe\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=df.columns)\n",
    "scaled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Transformation\n",
    "        Convert non-normal or non parametric data to normal/guassian or uniform distribution\n",
    "\n",
    "- NOTE: it comes under **Normalization**\n",
    "\n",
    "Distribution Conversion Image: https://scikit-learn.org/stable/modules/preprocessing.html#mapping-to-a-gaussian-distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate non-normal data (exponential Distribution)\n",
    "np.random.seed(0)\n",
    "df = np.random.exponential(size=1000, scale=2) # scale means: means of values will be 2\n",
    "# print(df)\n",
    "df = pd.DataFrame(df, columns=['values'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['values'], kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "pt_boxcox = PowerTransformer(method='box-cox', standardize=False)\n",
    "pt_yeo_johnson = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "qt_normal = QuantileTransformer(output_distribution='normal')\n",
    "\n",
    "# boxbcox k liay data must be postive\n",
    "df['Box_Cox'] = pt_boxcox.fit_transform(df[['values']] + 1)\n",
    "df['Yeo_Johnson'] = pt_yeo_johnson.fit_transform(df[['values']])\n",
    "df['Quantile'] = qt_normal.fit_transform(df[['values']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creata hostograms for all columns using sns.hist and kde=true use a for loop\n",
    "for col in df.columns:\n",
    "    sns.histplot(df[col], kde=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Normalization is a technique often applied as part of data preparation for machine learning. The goal of normalization is to change the values of numeric columns in the dataset to a common scale, without distorting differences in the ranges of values. For machine learning, every dataset does not require normalization. It is required only when features have different ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 Normalization:\n",
    "\n",
    "Rescales each sample (row) to have unit norm. This type of normalization is often used when dealing with text data.\n",
    "The L2 norm is calculated as the square root of the sum of the squared vector values.\n",
    "\n",
    "Used mostly in NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "data = [[1, 1, 1], # sqrt(1^2 + 1^2 + 1^2) = sqrt(3) --> normalized_value = 1 / sqrt(3) ≈ 0.57735027. OR FROM OUTPUT: sqrt((0.57735027)^2 + (0.57735027)^2 + (0.57735027)^2) = 1\n",
    "        [1, 1, 0], \n",
    "        [1, 0, 0]]\n",
    "normalizer = Normalizer(norm='l2')\n",
    "print(normalizer.fit_transform(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 Normalization:\n",
    "\n",
    "Also rescales each sample (row) but with a different approach, ensuring the sum of the absolute values is 1 in each row.\n",
    "The L1 norm is calculated as the sum of the absolute vector values.\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "data = [[1, 1, 1], [1, 1, 0], [1, 0, 0]]\n",
    "normalizer = Normalizer(norm='l1')\n",
    "print(normalizer.fit_transform(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Z-score normalization\n",
    "   1. Standard Scalar\n",
    "2. Min-Max normalization\n",
    "   1. Min-Max Scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# example dataset with skewed values\n",
    "df = { \"Values\": [1,5,10,20,50,100,200,500,1000,2000,5000,10000,20000,50000,100000,1000000]}\n",
    "df = pd.DataFrame(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log_values'] = np.log(df['Values'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    sns.histplot(df[col], kde=True)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
