{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Algorithm\n",
    "\n",
    "Decision Tree is a type of `supervised learning algorithm` that is mostly used in `classification problems`. \n",
    "It works for both `continuous` as well as `categorical output variables`. \n",
    "\n",
    "It is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/0*g4fblQAEbjS_arPX.png\" alt=\"Decision Tree Image\" \n",
    "        style=\"width: 500px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Important Terms for Decision Tree**\n",
    "\n",
    "- **Root Node**: It represents the entire dataset, which further gets divided into two or more homogeneous sets.\n",
    "- **Splitting**: It is a process of dividing a node into two or more sub-nodes.\n",
    "- **Decision Node**: When a sub-node splits into further sub-nodes, then it is called the decision node.\n",
    "- **Leaf/Terminal Node**: Nodes do not split is called Leaf or Terminal node.\n",
    "- **Pruning**: When we remove sub-nodes of a decision node, this process is called pruning. You can say the opposite process of splitting.\n",
    "- **Information Gain**: The `information gain` is based on the `decrease in entropy` after a dataset is split on an attribute. Constructing a decision tree is all about finding attribute that returns the highest information gain (i.e., the most homogeneous branches).\n",
    "- **Entropy**: A decision tree algorithm always tries to maximize Information Gain. The information gain is based on the decrease in entropy after a dataset is split on an attribute. Constructing a decision tree is all about finding attribute that returns the highest information gain (i.e., the most homogeneous branches).\n",
    "- **Gini Index**: Gini index says, if we select two items from a population at random then they must be of the same class and probability for this is 1 if the population is pure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For decision trees, features are not required to be on same scale. Means no need for StandardScalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Dataset\n",
    "# Let's say we have a dataset with two classes, A and B\n",
    "# Suppose in a dataset of 10 elements, 4 are of class A and 6 are of class B\n",
    "\n",
    "# Number of elements in each class\n",
    "n_A = 40\n",
    "n_B = 60\n",
    "total = n_A + n_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's calculate the proportions\n",
    "p_A = n_A / total\n",
    "p_B = n_B / total\n",
    "\n",
    "# print the proportions\n",
    "print(\"Proportion of A: \", p_A)\n",
    "print(\"Proportion of B: \", p_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy Calculate\n",
    "# Entropy is a measure of uncertainty\n",
    "entropy = -p_A * math.log2(p_A) - p_B * math.log2(p_B)\n",
    "print(\"Entropy: \", entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gini impurity\n",
    "# Gini impurity is a measure of misclassification\n",
    "gini = 1- p_A**2 - p_B**2\n",
    "print(\"Gini Impurity: \", gini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information Gain\n",
    "# Assuming a split on some feature divides the dataset into two subsets\n",
    "# Subset 1: 2 elements of A, 3 of B\n",
    "# Subset 2: 2 elements of A, 3 of B\n",
    "# Entropy and size for each subset\n",
    "n_1_A, n_1_B = 2, 3\n",
    "n_2_A, n_2_B = 2, 3\n",
    "\n",
    "p_1_A = n_1_A / (n_1_A + n_1_B)\n",
    "p_1_B = n_1_B / (n_1_A + n_1_B)\n",
    "entropy_1 = -p_1_A * math.log2(p_1_A) - p_1_B * math.log2(p_1_B) if p_1_A and p_1_B else 0\n",
    "\n",
    "p_2_A = n_2_A / (n_2_A + n_2_B)\n",
    "p_2_B = n_2_B / (n_2_A + n_2_B)\n",
    "entropy_2 = -p_2_A * math.log2(p_2_A) - p_2_B * math.log2(p_2_B) if p_2_A and p_2_B else 0\n",
    "\n",
    "# Calculating information gain\n",
    "info_gain = entropy - ((n_1_A + n_1_B) / total * entropy_1 + (n_2_A + n_2_B) / total * entropy_2)\n",
    "print(\"Information Gain: \", info_gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our example dataset with two classes (A and B), we have calculated the following values:\n",
    "\n",
    "1. **Entropy**: The calculated entropy of the dataset is approximately 0.971. This value indicates a moderate level of disorder in the dataset, considering that it's not very close to 0 (which would mean no disorder) and not at its maximum (which would mean complete disorder for a binary classification).\n",
    "\n",
    "2. **Gini Impurity**: The Gini impurity for the dataset is 0.48. This value, being less than 0.5, suggests some level of purity in the dataset but still indicates a mix of classes A and B.\n",
    "\n",
    "3. **Information Gain**: The information gain from the chosen split is 0.0. This result implies that the split did not reduce the entropy or disorder of the dataset. In other words, the split did not add any additional information that could help distinguish between classes A and B more effectively than before.\n",
    "\n",
    "These metrics provide insight into the nature of the dataset and the effectiveness of potential splits when constructing a decision tree. In practical applications, you would use these calculations to choose the best feature and split at each node in the tree to maximize the purity of the subsets created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Decision Tree Example in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "df = sns.load_dataset('titanic')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop deck column\n",
    "df.drop('deck', axis=1, inplace=True)\n",
    "\n",
    "#impute missing values of age, and fare using median\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df[['age', 'fare']] = imputer.fit_transform(df[['age', 'fare']]) # no need to impute fare here, as it has no missing value\n",
    "\n",
    "# impute missing values of embark and embarked_town using mode\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "df[['embark_town', 'embarked']] = imputer.fit_transform(df[['embark_town', 'embarked']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the categorical and object variables using for loop and labelencoder\n",
    "le = LabelEncoder()\n",
    "for col in df.select_dtypes(include=['category', 'object']):\n",
    "    df[col] = le.fit_transform(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into X and y\n",
    "X = df.drop(['survived', 'alive'], axis=1)\n",
    "y = df['survived']\n",
    "# split the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and train teh model with pred\n",
    "model = DecisionTreeClassifier(criterion='entropy', # gini is default\n",
    "                               random_state=42,\n",
    "                               max_depth=5, # max_depth is optional - it limits the depth of the tree\n",
    "                               ) \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# predict the model\n",
    "y_pred = model.predict(X_test)\n",
    "# evaluate the model\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# plot confusion matrix\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision tree\n",
    "\n",
    "plt.figure(dpi=600, facecolor='white', edgecolor='none', figsize=(12, 8))\n",
    "plot_tree(model, filled=True, feature_names=X.columns) # class_names=['Not Survived', 'Survived']\n",
    "plt.title('Decision Tree Visualization')\n",
    "\n",
    "# save the plot DT visualization\n",
    "# plt.savefig('decision_tree_visualization.png', dpi=600, bbox_inches='tight') # format='tiff'\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the decision tree classifier\n",
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(model, out_file='./saved_models/03_Decision_Tree.dot', feature_names=X.columns, filled=True, rounded=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Alert: \n",
    "### Use Decision Tree as a Regression Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# load the dataset\n",
    "tips = sns.load_dataset('tips')\n",
    "# print(tips.head())\n",
    "\n",
    "# split the data into X and y\n",
    "X = tips[['total_bill']]\n",
    "y = tips['tip']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # split the data into train and test\n",
    "\n",
    "# create and train teh model with pred\n",
    "model = DecisionTreeRegressor(random_state=42,\n",
    "                               min_samples_leaf=0.1, # min_samples_leaf is optional - it limits the minimum samples required to be at a leaf node\n",
    "                               max_depth=5, # max_depth is optional - it limits the depth of the tree\n",
    "                               ) \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# predict the model\n",
    "y_pred = model.predict(X_test)\n",
    "# evaluate the model\n",
    "print(mean_absolute_error(y_test, y_pred))\n",
    "print(mean_squared_error(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
