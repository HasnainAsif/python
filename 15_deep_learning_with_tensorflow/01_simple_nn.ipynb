{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Network in Python using TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><span style=\n",
    "  \"\n",
    "  font-size: 50px; \n",
    "  font-weight: bold;\n",
    "  color: Yellow;\n",
    "  text-decoration: underline;\n",
    "  text-decoration-color: White;\n",
    "  \"\n",
    ">\n",
    "   Activation Functions <!--  paste your text -->\n",
    "  </span></center>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "| Activation Function | Common Use | Advantages | Disadvantages |\n",
    "|---|---|---|---|\n",
    "| Sigmoid / Logistic | Predicting the probability as output | Outputs between 0 and 1, useful for binary classification, differentiable and provides smooth gradient | Suffers from vanishing gradient problem, output not symmetric around zero |\n",
    "| Tanh (Hyperbolic Tangent) | Hidden layers of neural network | Output is zero-centered, helps centering the data | Suffers from vanishing gradient problem, gradient is steeper compared to sigmoid |\n",
    "| ReLU (Rectified Linear Unit) | Hidden layers of neural network | Computationally efficient, accelerates convergence of gradient descent | Suffers from \"dying ReLU\" problem, all negative input values become zero |\n",
    "| Leaky ReLU | To avoid dying ReLU problem | Enables backpropagation for negative input values, avoids dead neurons | Predictions may not be consistent for negative input values, learning of model parameters is time-consuming |\n",
    "| Parametric ReLU | When Leaky ReLU fails at solving dead neurons problem | Slope of the negative part can be learnt during backpropagation | Performance varies depending on the value of slope parameter 'a' |\n",
    "| Exponential Linear Units (ELUs) | Modifies slope of the negative part of the function | Smoothly approaches the value of -α for negative inputs, avoids dead ReLU problem | Increases computational time, no learning of the 'a' value, suffers from exploding gradient problem |\n",
    "| Softmax | Output layer of classifier to represent probability distribution | Output is a probability distribution over 'n' classes | Limitations when dealing with non-exclusive classes |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas numpy matplotlib seaborn plotly scikit-learn scipy tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps before Creating a Neural Network (Preprocess the Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load Titanic dataset\n",
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "# Preprocessing\n",
    "# Dropping rows with missing 'age' and 'embarked' values\n",
    "titanic.dropna(subset=['age', 'embarked'], inplace=True)\n",
    "\n",
    "# Converting categorical variables to dummy variables\n",
    "titanic = pd.get_dummies(titanic, columns=['sex', 'embarked', 'class', 'who', 'deck'], drop_first=True)\n",
    "\n",
    "# Selecting features and target\n",
    "X = titanic.drop(['survived', 'alive', 'embark_town', 'adult_male', 'alone'], axis=1)\n",
    "y = titanic['survived']\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardizing the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the layers of the model\n",
    "input_layer = tf.keras.layers.Dense(10, # Number of neurons (units) in this layer — this layer will output a vector of size 10\n",
    "                                    activation='relu', # The activation function for this layer, which introduces non-linearity to the model\n",
    "                                    input_shape=(X_train.shape[1],) # Shape of the input vector. i.e; number of features in X_train\n",
    "                                    )\n",
    "# So this input layer will accept an input vector of size 18 (i.e., number of features), and output a vector of size 10\n",
    "\n",
    "# Most of the time, \"units\" in the input layer are equal to the number of features in the dataset. It could also be multiple of the number of features, like 2x, 3x, etc. depending on the complexity of the dataset.\n",
    "\n",
    "############ ANOTHER WAY TO DEFINE INPUT LAYER ############\n",
    "# input_layer = tf.keras.layers.Input(shape=(X_train.shape[1],)) # Input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_layer = tf.keras.layers.Dense(10, activation='relu') # hidden layer\n",
    "\n",
    "# number of hidden layers and neurons also depends on number of features and complexity of the dataset.\n",
    "\n",
    "# Mostly non-linear activation functions are used in hidden layers. Most commenly used is relu and then, sigmoid and tanh.\n",
    "# Usually, all hidden layers contain same activation function.\n",
    "# Neural network types MLP and CNN make use of ReLU activation function while Recurrent networks still commonly use Tanh and Sigmoid activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = tf.keras.layers.Dense(1, # Number of neurons (units) in this layer = 1 (because you're doing binary classification)\n",
    "                                     activation='sigmoid' # The activation function for this layer, which is suitable for binary classification tasks (like 0 or 1)\n",
    "                                     )\n",
    "# So this layer takes input (10 values from previous layer), combines them using weights and bias, applies sigmoid, and gives output between 0 and 1.\n",
    "\n",
    "# For regression problems, \"units\" in the output layer is usually 1, and the activation function is linear (i.e., no activation function).\n",
    "# For binary classification problems, \"units\" in the output layer is usually 1, and the activation function is sigmoid.\n",
    "# For multi-class classification problems, \"units\" in the output layer is equal to the number of classes, and the activation function is softmax. (multi-class means more than 2 classes, like: 0, 1, 2, 3 or dog, cat, bird, etc.)\n",
    "# For multi-label classification problems, \"units\" in the output layer is equal to the number of labels, and the activation function is sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax is especially used in output layer of multi-class classification problems, where it converts the output into probabilities for each class.\n",
    "# ReLU is most popularly used for hidden layers, as it helps the model learn complex patterns by introducing non-linearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the layers into a model\n",
    "model = tf.keras.models.Sequential([input_layer, \n",
    "                                    # hidden_layer, \n",
    "                                    output_layer])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', # Optimizer used to update the weights during training. Adam is one of the best choices for most tasks.\n",
    "              loss='binary_crossentropy', # Loss function used to measure prediction error. Binary cross-entropy is suitable for binary classification tasks.\n",
    "              metrics=['accuracy']) # accuracy tells you how much predictions were correct.\n",
    "\n",
    "### Some other optimizers:\n",
    "# tf.keras.optimizers.SGD(0.001,0.9) --> SGD (Stochastic Gradient Descent): A basic optimizer that updates weights based on the gradient of the loss function. It can be slow to converge.\n",
    "\n",
    "### Some other loss functions:\n",
    "# 'mean_squared_error' --> Used for regression tasks where the goal is to minimize the squared difference between predicted and actual values.\n",
    "# 'categorical_crossentropy' --> Used for multi-class classification tasks where the target variable has more than two classes. - When target labels are one-hot encoded vectors.\n",
    "# 'sparse_categorical_crossentropy' --> Similar to categorical_crossentropy but used when target labels are integers (i.e; 0,1 2,3) instead of one-hot encoded vectors.\n",
    "\n",
    "### Some other metrics:\n",
    "# 'MAE' (Mean Absolute Error) --> Used for regression tasks to measure the average absolute difference between predicted and actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Training the model\n",
    "model.fit(X_train, y_train, \n",
    "    epochs=100, # Number of times the model will go through the entire training dataset\n",
    "    batch_size=32, # Number of samples processed per iteration. A smaller batch size means more updates per epoch, but slower training.\n",
    "    verbose=1, # Shows logs during training. [0, 1, 2] where 0 = silent, 1 = progress bar, 2 = one line per epoch\n",
    "    # validation_data=(X_test, y_test), # Data on which to evaluate the model after each epoch. Helps monitor overfitting.\n",
    "    )\n",
    "\n",
    "# if dataset is of 3200 rows then, no. of iterations per epoch = 3200/32 = 100\n",
    "# hence, total no. of iterations = (no. of epochs) * (no. of iterations per epoch) = 100 * 100 = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Term           | Meaning                                                         |\n",
    "| -------------- | --------------------------------------------------------------- |\n",
    "| **epoch**      | 1 full pass over the entire training dataset                    |\n",
    "| **batch size** | How many samples the model looks at **before updating weights** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets say, we have 3200 training examples in X_train\n",
    "\n",
    "<img src=\"../0_resources/images/epoch_batchsize.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "y_pred = model.predict(X_test)  # Predicting the first 5 samples in the test set\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see all the steps in action within one snippet of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "#remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load Titanic dataset\n",
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "# Preprocessing\n",
    "# Dropping rows with missing 'age' and 'embarked' values\n",
    "titanic.dropna(subset=['age', 'embarked'], inplace=True)\n",
    "\n",
    "# Converting categorical variables to dummy variables\n",
    "titanic = pd.get_dummies(titanic, columns=['sex', 'embarked', 'class', 'who', 'deck'], drop_first=True)\n",
    "\n",
    "# Selecting features and target\n",
    "X = titanic.drop(['survived', 'alive', 'embark_town', 'adult_male', 'alone'], axis=1)\n",
    "y = titanic['survived']\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardizing the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Building the model\n",
    "input_layer = tf.keras.layers.Dense(10, activation='relu', input_shape=(X_train.shape[1],)) # input layer\n",
    "# input_layer = tf.keras.layers.Input(shape=(X_train.shape[1],)) # another way to initialize input layer\n",
    "# hidden_layer = tf.keras.layers.Dense(10, activation='relu') # hidden layer\n",
    "output_layer = tf.keras.layers.Dense(1, activation='sigmoid') # output layer\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential([input_layer, \n",
    "                                    # hidden_layer, \n",
    "                                    output_layer])\n",
    "\n",
    "######## Another way to Define the model ########\n",
    "# model = tf.keras.models.Sequential()\n",
    "# model.add(tf.keras.layers.Dense(10, activation='relu', input_shape=(X_train.shape[1],))) # input layer\n",
    "# model.add(tf.keras.layers.Dense(1, activation='sigmoid')) # output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "r = model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluating the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Assignment:** Plot the Training and Validation Accuracy and Loss for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training and validation, loss and accuracy for each epoch\n",
    "import matplotlib.pyplot as plt\n",
    "history = model.history.history # another way to get history --> r.history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "print(history['accuracy']) # training accuracy\n",
    "print(history['loss']) # training loss\n",
    "\n",
    "print(history['val_accuracy']) # validation accuracy - This values will exist if \"validation_data\" is provided during model.fit\n",
    "print(history['val_loss']) # validation loss - This values will exist if \"validation_data\" is provided during model.fit\n",
    "\n",
    "# plot one line for accuracy and one for loss on the same graph without subplots\n",
    "plt.plot(history['accuracy'], label='Accuracy')\n",
    "plt.plot(history['loss'], label='Loss')\n",
    "plt.title('Model Accuracy and Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import tensorflow.keras\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "import platform\n",
    "\n",
    "print(f\"Python Platform: {platform.platform()}\")\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "print(f\"SciPy {sp.__version__}\")\n",
    "gpu = len(tf.config.list_physical_devices('GPU'))>0\n",
    "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
