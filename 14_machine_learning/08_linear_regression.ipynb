{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression | Pipeline | Save and Load a trained Model | Model Evaluation\n",
    "\n",
    "Linear regression is a linear approach to modeling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). The case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression.\n",
    "\n",
    "## Simple Linear Regression\n",
    "\n",
    "Simple linear regression is an approach for predicting a response using a single feature. It is assumed that the two variables are linearly related. Hence, we try to find a linear function that predicts the response value(y) as accurately as possible as a function of the feature or independent variable(x).\n",
    "\n",
    "The linear function can be represented as:\n",
    "$$\n",
    "y = mx + c\n",
    "$$\n",
    "Where:\n",
    "- y is the response\n",
    "- x is the feature\n",
    "- m is the slope\n",
    "- c is the intercept\n",
    "- m and c are the coefficients of the model\n",
    "  \n",
    "The model is trained on a dataset with features(x) and target(y). It learns the values of coefficients m and c that minimize the difference between actual values in the dataset and predicted values by the model. The difference between actual values and predicted values is known as the cost function. The model optimizes the cost function using the gradient descent algorithm.\n",
    "\n",
    "## Multiple Linear Regression\n",
    "\n",
    "Multiple linear regression is an extension of simple linear regression as it takes more than one feature to predict the response. The linear function can be represented as:\n",
    "$$\n",
    "y = m1x1 + m2x2 + m3x3 + ... + c\n",
    "$$\n",
    "Where:\n",
    "- y is the response\n",
    "- x1, x2, x3, ... are the features\n",
    "- m1, m2, m3, ... are the coefficients of the model\n",
    "- c is the intercept\n",
    "- m1, m2, m3, ... and c are the coefficients of the model\n",
    "\n",
    "The model is trained on a dataset with features(x) and target(y). It learns the values of coefficients m1, m2, m3, ... and c that minimize the difference between actual values in the dataset and predicted values by the model. The difference between actual values and predicted values is known as the cost function. The model optimizes the cost function using the gradient descent algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data\n",
    "df = sns.load_dataset('tips')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot with regression line\n",
    "sns.lmplot(x='total_bill', y='tip', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the data\n",
    "scalar = StandardScaler()\n",
    "df[['total_bill', 'tip']] = scalar.fit_transform(df[['total_bill', 'tip']])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='total_bill', y='tip', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the features X and the target/labels y\n",
    "X = df[['total_bill']]\n",
    "y = df['tip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the model\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# train the model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As, eq. of line is y = mx + b. m is coefficient and b is constant(intercept)\n",
    "print(\"coefficients: \", model.coef_) # coefficient is m\n",
    "print(\"intercept: \", model.intercept_) # intercept is b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([[15]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation the model\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error, mean_absolute_error\n",
    "print('MSE: ', mean_squared_error(y_test, y_pred))\n",
    "print('R2: ', r2_score(y_test, y_pred))\n",
    "print('MAPE: ', mean_absolute_percentage_error(y_test, y_pred))\n",
    "print('MAE: ', mean_absolute_error(y_test, y_pred))\n",
    "# root mean squared error\n",
    "print('RMSE: ', np.sqrt(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Save and load a trained Machine Learning model**\n",
    "\n",
    "After training a machine learning model, it is important to save the model to a file. This is because training a machine learning model can be computationally expensive and time-consuming. Saving the model to a file allows us to use the trained model in the future without having to retrain it. In this notebook, we will see how to save and load a trained machine learning model using the `pickle` library in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "import pickle\n",
    "# create folder to save the model if exists remove it\n",
    "import os\n",
    "if not os.path.exists('saved_models'):\n",
    "    os.makedirs('saved_models')\n",
    "# save the model\n",
    "pickle.dump(model, open('./saved_models/01_linear_regression.pkl', 'wb')) # mode --> wb (write binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "import pickle\n",
    "model_load = pickle.load(open('./saved_models/01_linear_regression.pkl', 'rb')) # mode --> rb (read binary)\n",
    "model_load.predict([[1500.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilinear Regression\n",
    "\n",
    "Multi-linear regression is a type of regression analysis that is used to predict the relationship between two or more independent variables and one dependent variable. In this notebook, we will use the `sklearn` library to create a multi-linear regression model and save it to a file. We will then load the model from the file and use it to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "#data pre processing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "#ignore warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# load the data\n",
    "df = sns.load_dataset('tips')\n",
    "df.head()\n",
    "\n",
    "X = df[['total_bill', 'size', 'day']]\n",
    "y = df['tip']\n",
    "\n",
    "# preprocess the data\n",
    "scalar = StandardScaler()\n",
    "X[['total_bill', 'size']] = scalar.fit_transform(X[['total_bill', 'size']])\n",
    "le = LabelEncoder()\n",
    "X['day'] = le.fit_transform(X['day'])\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)\n",
    "\n",
    "# call the model\n",
    "model = LinearRegression()\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# eq. of line is y = (m1x1 + m2x2 + m3x3) + b --> we have 3 features: total_bill, size, day\n",
    "print(model.coef_)  # coefficients\n",
    "print(model.intercept_)  # intercept\n",
    "\n",
    "# metric to evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(f\"Mean Squared Error: {mean_squared_error(y_test, y_pred)}\")\n",
    "print(f\"R2 Score: {r2_score(y_test, y_pred)}\")\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error(y_test, y_pred)}\")\n",
    "print(f\"Mean Absolute Percentage Error: {mean_absolute_percentage_error(y_test, y_pred)}\")\n",
    "# root mean squared error\n",
    "print(f\"Root Mean Squared Error: {np.sqrt(mean_squared_error(y_test, y_pred))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pipleline\n",
    "\n",
    "In this notebook, we will use the `Pipeline` class from the `sklearn` library to create a multi-linear regression model. The `Pipeline` class allows us to chain multiple estimators into a single estimator. This makes it easier to work with multiple steps in a machine learning workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "#data pre processing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "#ignore warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# load the data\n",
    "df = sns.load_dataset('tips')\n",
    "# separate the features X and the target/labels y\n",
    "X = df[['total_bill', 'size', 'day']]\n",
    "y = df['tip']\n",
    "\n",
    "# numeric features\n",
    "numeric_features = ['total_bill', 'size']\n",
    "# categorical features\n",
    "categorical_features = ['day']\n",
    "\n",
    "# preprocess the data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)\n",
    "\n",
    "# pipeline\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', LinearRegression())\n",
    "    ]\n",
    ")\n",
    "\n",
    "# fit the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# metric to evaluate the model\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(f\"Mean Squared Error: {mean_squared_error(y_test, y_pred)}\")\n",
    "print(f\"R2 Score: {r2_score(y_test, y_pred)}\")\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error(y_test, y_pred)}\")\n",
    "print(f\"Mean Absolute Percentage Error: {mean_absolute_percentage_error(y_test, y_pred)}\")\n",
    "# root mean squared error\n",
    "print(f\"Root Mean Squared Error: {np.sqrt(mean_squared_error(y_test, y_pred))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go for big data\n",
    "\n",
    "We will use diamonds dataset from seaborn library. The dataset contains the following columns:\n",
    "- `carat`: weight of the diamond (0.2--5.01)\n",
    "- `cut`: quality of the cut (Fair, Good, Very Good, Premium, Ideal)\n",
    "- `color`: diamond colour, from J (worst) to D (best)\n",
    "- `clarity`: a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))\n",
    "- `depth`: total depth percentage = z / mean(x, y) = 2 * z / (x + y) (43--79)\n",
    "- `table`: width of top of diamond relative to widest point (43--95)\n",
    "- `price`: price in US dollars (\\$326--\\$18,823)\n",
    "- `x`: length in mm (0--10.74)\n",
    "- `y`: width in mm (0--58.9)\n",
    "- `z`: depth in mm (0--31.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "#data pre processing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "#ignore warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# load the data\n",
    "df = sns.load_dataset('diamonds')\n",
    "\n",
    "# separate the features X and the target/labels y\n",
    "X = df.drop('price', axis=1)\n",
    "y = df['price']\n",
    "\n",
    "# numeric features\n",
    "numeric_features = ['carat', 'depth', 'table', 'x', 'y', 'z']\n",
    "# categorical features\n",
    "categorical_features = ['cut', 'color', 'clarity']\n",
    "\n",
    "# preprocess the data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)\n",
    "\n",
    "# pipeline\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', LinearRegression())\n",
    "    ]\n",
    ")\n",
    "\n",
    "# fit the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# metric to evaluate the model\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(f\"Mean Squared Error: {mean_squared_error(y_test, y_pred)}\")\n",
    "print(f\"R2 Score: {r2_score(y_test, y_pred)}\")\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error(y_test, y_pred)}\")\n",
    "print(f\"Mean Absolute Percentage Error: {mean_absolute_percentage_error(y_test, y_pred)}\")\n",
    "# root mean squared error\n",
    "print(f\"Root Mean Squared Error: {np.sqrt(mean_squared_error(y_test, y_pred))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of the model Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "The Metrics indicate the performance of your regression model. \n",
    "\n",
    "Here's a brief interpretation:\n",
    "\n",
    "- **Mean Squared Error (MSE)**: 1288813.63 - This value represents the average of the squares of the errors. A lower MSE indicates a better fit, but the value itself is not very interpretable without context. \n",
    "    > `e.g:` if the target variable is in the range of 0-100, an MSE of 1288813.63 is high, but if the target variable is in the range of 100000-1000000, then the MSE is low. \n",
    "\n",
    "- **R2 Score**: 0.9189 - This value indicates that approximately 91.89% of the variance in the dependent variable (price) is predictable from the independent variables. An R2 score close to 1 indicates a good fit.\n",
    "    > `e.g:` if the R2 score is 0.9189, it means that 91.89% of the variance in the price can be explained by the independent variables in the model.\n",
    "\n",
    "\n",
    "- **Mean Absolute Error (MAE)**: 736.91 - This value represents the average absolute difference between the predicted and actual values. Lower values indicate better performance.\n",
    "    > `e.g:` if the MAE is 736.91, it means that, on average, the model's predictions are off by $736.91.\n",
    "\n",
    "\n",
    "- **Mean Absolute Percentage Error (MAPE)**: 0.3951 - This value represents the average absolute percentage difference between the predicted and actual values. Lower values indicate better performance.\n",
    "    > `e.g:` if the MAPE is 0.3951, it means that, on average, the model's predictions are off by 39.51%.\n",
    "\n",
    "- **Root Mean Squared Error (RMSE)**: 1135.26 - This value is the square root of the MSE and provides a measure of the average magnitude of the error. Lower values indicate better performance.\n",
    "    > `e.g:` if the RMSE is 1135.26, it means that, on average, the model's predictions are off by $1135.26.\n",
    "\n",
    "Overall, the R2 score of 0.9189 suggests that your model explains a significant portion of the variance in the data, which is a good sign. However, the MSE, MAE, and RMSE values are relatively high, indicating that there is still room for improvement in the model's accuracy.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this notebook, we created a multi-linear regression model using the `sklearn` library and saved it to a file. We then loaded the model from the file and used it to make predictions. We also evaluated the model's performance using various metrics such as MSE, R2 score, MAE, MAPE, and RMSE. The results indicate that the model explains a significant portion of the variance in the data but has room for improvement in terms of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **15 Ways to improve the ML Model's performance?**\n",
    "\n",
    "1. **Feature Engineering**: Create new features that capture additional information from the data.\n",
    "2. **Hyperparameter Tuning**: Optimize the model's hyperparameters to improve performance.\n",
    "3. **Regularization**: Apply regularization techniques to prevent overfitting.\n",
    "4. **Ensemble Methods**: Use ensemble methods such as Random Forest or Gradient Boosting to improve predictive performance.\n",
    "5. **Cross-Validation**: Use cross-validation to assess the model's performance more accurately.\n",
    "6. **Feature Selection**: Identify and select the most relevant features for the model.\n",
    "7. **Data Preprocessing**: Clean and preprocess the data to improve model performance.\n",
    "8. **Model Selection**: Experiment with different regression models to find the best fit for the data.\n",
    "9. **Error Analysis**: Analyze the model's errors to identify patterns and areas for improvement.\n",
    "10. **Domain Knowledge**: Incorporate domain knowledge to improve the model's predictive power.\n",
    "11. **Data Augmentation**: Increase the size of the training data through data augmentation techniques.\n",
    "12. **Model Stacking**: Combine multiple models to improve predictive performance.\n",
    "13. **Model Interpretation**: Interpret the model's predictions to gain insights into the data and improve performance.\n",
    "14. **Model Deployment**: Deploy the model in a production environment and monitor its performance over time.\n",
    "15. **Feedback Loop**: Incorporate feedback from users and stakeholders to continuously improve the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-family: 'poppins'; font-weight: bold; color: Green;\">üë®‚ÄçüíªAuthor: Dr. Muhammad Aammar Tufail</h2>\n",
    "\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-Profile-blue?style=for-the-badge&logo=github)](https://github.com/AammarTufail) \n",
    "[![Kaggle](https://img.shields.io/badge/Kaggle-Profile-blue?style=for-the-badge&logo=kaggle)](https://www.kaggle.com/muhammadaammartufail) \n",
    "[![LinkedIn](https://img.shields.io/badge/LinkedIn-Profile-blue?style=for-the-badge&logo=linkedin)](https://www.linkedin.com/in/dr-muhammad-aammar-tufail-02471213b/)  \n",
    "\n",
    "[![YouTube](https://img.shields.io/badge/YouTube-Profile-red?style=for-the-badge&logo=youtube)](https://www.youtube.com/@codanics) \n",
    "[![Facebook](https://img.shields.io/badge/Facebook-Profile-blue?style=for-the-badge&logo=facebook)](https://www.facebook.com/aammar.tufail) \n",
    "[![TikTok](https://img.shields.io/badge/TikTok-Profile-black?style=for-the-badge&logo=tiktok)](https://www.tiktok.com/@draammar)  \n",
    "\n",
    "[![Twitter/X](https://img.shields.io/badge/Twitter-Profile-blue?style=for-the-badge&logo=twitter)](https://twitter.com/aammar_tufail) \n",
    "[![Instagram](https://img.shields.io/badge/Instagram-Profile-blue?style=for-the-badge&logo=instagram)](https://www.instagram.com/aammartufail/) \n",
    "[![Email](https://img.shields.io/badge/Email-Contact%20Me-red?style=for-the-badge&logo=email)](mailto:aammar@codanics.com)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to improve the ML model Performance?\n",
    "\n",
    "#### improve by Changing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "#data pre processing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# decision tree regressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "#ignore warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# load the data\n",
    "df = sns.load_dataset('diamonds')\n",
    "\n",
    "# separate the features X and the target/labels y\n",
    "X = df.drop('price', axis=1)\n",
    "y = df['price']\n",
    "\n",
    "# numeric features\n",
    "numeric_features = ['carat', 'depth', 'table', 'x', 'y', 'z']\n",
    "# categorical features\n",
    "categorical_features = ['cut', 'color', 'clarity']\n",
    "\n",
    "# preprocess the data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)\n",
    "\n",
    "# pipeline\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', DecisionTreeRegressor()) # Model Changed\n",
    "    ]\n",
    ")\n",
    "\n",
    "# fit the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# metric to evaluate the model\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(f\"Mean Squared Error: {mean_squared_error(y_test, y_pred)}\")\n",
    "print(f\"R2 Score: {r2_score(y_test, y_pred)}\")\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error(y_test, y_pred)}\")\n",
    "print(f\"Mean Absolute Percentage Error: {mean_absolute_percentage_error(y_test, y_pred)}\")\n",
    "# root mean squared error\n",
    "print(f\"Root Mean Squared Error: {np.sqrt(mean_squared_error(y_test, y_pred))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "#data pre processing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# decision tree regressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# random forest regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "#ignore warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# load the data\n",
    "df = sns.load_dataset('diamonds')\n",
    "\n",
    "# separate the features X and the target/labels y\n",
    "X = df.drop('price', axis=1)\n",
    "y = df['price']\n",
    "\n",
    "# numeric features\n",
    "numeric_features = ['carat', 'depth', 'table', 'x', 'y', 'z']\n",
    "# categorical features\n",
    "categorical_features = ['cut', 'color', 'clarity']\n",
    "\n",
    "# preprocess the data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)\n",
    "\n",
    "# pipeline\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', RandomForestRegressor()) # Model Changed\n",
    "    ]\n",
    ")\n",
    "\n",
    "# fit the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# metric to evaluate the model\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(f\"Mean Squared Error: {mean_squared_error(y_test, y_pred)}\")\n",
    "print(f\"R2 Score: {r2_score(y_test, y_pred)}\")\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error(y_test, y_pred)}\")\n",
    "print(f\"Mean Absolute Percentage Error: {mean_absolute_percentage_error(y_test, y_pred)}\")\n",
    "# root mean squared error\n",
    "print(f\"Root Mean Squared Error: {np.sqrt(mean_squared_error(y_test, y_pred))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install xgboost --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "#data pre processing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# decision tree regressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# random forest regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# XGboost models\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "#ignore warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# load the data\n",
    "df = sns.load_dataset('diamonds')\n",
    "\n",
    "# separate the features X and the target/labels y\n",
    "X = df.drop('price', axis=1)\n",
    "y = df['price']\n",
    "\n",
    "# numeric features\n",
    "numeric_features = ['carat', 'depth', 'table', 'x', 'y', 'z']\n",
    "# categorical features\n",
    "categorical_features = ['cut', 'color', 'clarity']\n",
    "\n",
    "# preprocess the data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)\n",
    "\n",
    "# pipeline\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', XGBRegressor()) # Model Changed\n",
    "    ]\n",
    ")\n",
    "\n",
    "# fit the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# metric to evaluate the model\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(f\"Mean Squared Error: {mean_squared_error(y_test, y_pred)}\")\n",
    "print(f\"R2 Score: {r2_score(y_test, y_pred)}\")\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error(y_test, y_pred)}\")\n",
    "print(f\"Mean Absolute Percentage Error: {mean_absolute_percentage_error(y_test, y_pred)}\")\n",
    "# root mean squared error\n",
    "print(f\"Root Mean Squared Error: {np.sqrt(mean_squared_error(y_test, y_pred))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLR\n",
    "- Mean Squared Error: 1288813.6340378197\n",
    "- R2 Score: 0.9189263314059744\n",
    "- Mean Absolute Error: 736.9080459770115\n",
    "- Mean Absolute Percentage Error: 0.3950714565701874\n",
    "- Root Mean Squared Error: 1135.2592805336672"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "- Mean Squared Error: 549017.3375509826\n",
    "- R2 Score: 0.9654637035941873\n",
    "- Mean Absolute Error: 359.56655543196143\n",
    "- Mean Absolute Percentage Error: 0.08604352592144883\n",
    "- Root Mean Squared Error: 740.9570416366813"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "- Mean Squared Error: 307525.9870314578\n",
    "- R2 Score: 0.9806548756948459\n",
    "- Mean Absolute Error: 272.0160216422127\n",
    "- Mean Absolute Percentage Error: 0.06513102240582337\n",
    "- Root Mean Squared Error: 554.5502565425949\n",
    "\n",
    "# XGBoost\n",
    "- Mean Squared Error: 318286.3109612889\n",
    "- R2 Score: 0.9799779653549194\n",
    "- Mean Absolute Error: 285.6134524615906\n",
    "- Mean Absolute Percentage Error: 0.07398491535044392\n",
    "- Root Mean Squared Error: 564.168690163934"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
